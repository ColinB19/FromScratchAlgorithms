{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest-Neighbors from Scratch\n",
    "\n",
    "Let's look at coding up one of the simplest machine learning (ML) algorithms: k-Nearest-Neighbors (kNN). kNN is an ML algorithm that classifies an object based on some distance metric from other labeled objects. The idea is, for a given item, you will calculate the distance between that item and all other items. You then take the k-nearest-neighbors and then they vote on what the item label should be. For example if you set $k=5$ and three of those neighbors have `class = 'A'` and the other two have `class = 'B'` then the new item will be labeled \n",
    "as `class = 'A'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A distance metric\n",
    "In order for kNN to work, we need to establish a distance metric. This metric quantifies how close any two items are. A really sraight forward example is points on a map. Say I have two coordinates on a map and I want to quantify how close these points are. What is the best way to quantify this? Well, if the map is nice and flat then the distance between the two points seems to be a good metric. How do I compute this? Well you define a coordinate system, subtract the position vectors of the two points and then find the magnitude of the resulting vector. This is called the **Euclidean distance** and it is what we will use here. For two $N$-dimensional vectors $\\vec{u}$ and $\\vec{v}$, the **Euclidean distance** between them is defined as\n",
    "$$\n",
    "d_{\\vec{u}\\to\\vec{v}} = \\sqrt{\\sum_{i=1}^{N} (v_i - u_i)^2}.\n",
    "$$\n",
    "\n",
    "We will take advantage of `np.linalg` to compute this quantity on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between (1,0) and (1,0) is 0.0.\n",
      "The distance between (1,0) and (-1,0) is 2.0.\n"
     ]
    }
   ],
   "source": [
    "# let's define our euclidean distance metric using lambdas\n",
    "euc_dist = lambda x: np.linalg.norm(x[1]-x[0])\n",
    "\n",
    "# now let's check if this works. The distance between (1,0) and (1,0) should be zero and the distance between\n",
    "# (1,0) and (-1,0) should be 2!\n",
    "v1 = np.array([1,0])\n",
    "v2 = np.array([-1,0])\n",
    "\n",
    "check = '''The distance between (1,0) and (1,0) is {}.\n",
    "The distance between (1,0) and (-1,0) is {}.'''.format(euc_dist([v1,v1]),euc_dist([v1,v2]))\n",
    "\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing kNN\n",
    "\n",
    "Now that we have a distance metric, we should code up our kNN algorithm. Let's write three functions to do this:\n",
    "\n",
    "1. `predict_set`: This function will handle going through your test set and predicting on each row. It will output\n",
    "    the accuracy over the entire test set. You must pass the training and testing set. The optional inputs are `k` and `distance_metric`. `k` is the number of neighbors considered in the voting process, it's default is 3. `distance_metric` takes in the function that defines the distance metric you want to use, it's default is the Euclidean distance.\n",
    "    \n",
    "2. `predict`: This function will figure out the distances and then run the `vote` function to get a prediction. `train`, `k`, and `distance_metric` are all the same as in `predict_set`; however, since kNN is insance based we only can pass it one test instance at a time. So here we pass it an individual test insance as `test_vec`. The parameter `reverse` is only `True` if the `distance_metric` is the cosine distance. This will then output a tuple with the first element being the test instance and the second element being the prediction. \n",
    "\n",
    "3. `vote`: This function actually performs the meat of the kNN algorithm. It takes in the `distance` established in the `predict` function and the parameter `k`. It will then find the k-nearest-neighbors and have them vote on the class of the instance. If there is a tie `k` is reduced and the voting happens again; if not, the vote is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_set(train, test, k=3, distance_metric = euc_dist):\n",
    "    \n",
    "    # if we use the cosine distance metric then the order of closeness is reversed, i.e. 1 is closer than 0.\n",
    "    if (distance_metric == cosine_distance):\n",
    "        reverse = True\n",
    "    else:\n",
    "        reverse = False\n",
    "    \n",
    "    # now we are just looping through the rows in the test set and checking wether or not the prediction is correct.\n",
    "    correct = 0\n",
    "    for index, row in test.iterrows():\n",
    "        prediction = predict(train,row[:-1], k, distance_metric, reverse)\n",
    "        if (prediction[-1] == row[-1]):\n",
    "            correct += 1\n",
    "    print(\"Congrats, you have {:.2%} accuracy!\".format(correct/len(test)))\n",
    "    \n",
    "    return correct/len(test)\n",
    "\n",
    "def predict(train, test_vec, k=5, distance_metric = euc_dist, reverse = False):\n",
    "    distance = []\n",
    "\n",
    "    # compute distance metric for every instance of the training set\n",
    "    for index, row in train.iterrows():\n",
    "        dist = distance_metric([row[:-1],test_vec])\n",
    "        distance.append([row,dist])\n",
    "        \n",
    "    # here we're just sorting all the distances from the 'closest' to the 'furthest'\n",
    "    distance.sort(key = lambda x: x[1], reverse=reverse)\n",
    "\n",
    "    # now look at the k-nearest-neighbors and have them vote on the label!\n",
    "    pred = vote(distance, k)\n",
    "    \n",
    "    return (test_vec,pred[0])\n",
    "\n",
    "\n",
    "def vote(distance, k):\n",
    "    \n",
    "    # taking the k-nearest-neighbors then figuring out how many times each class appears in that list\n",
    "    knn = distance[:k]\n",
    "    occurances = Counter(row[0]['class'] for row in knn)\n",
    "    occurances = [(key, value) for key, value in sorted(occurances.items(), key=lambda item: item[1], reverse = True)]\n",
    "    \n",
    "    # if we have a tie we want to reduce k until we break the tie!\n",
    "    if ((len(occurances) > 1) and (occurances[0][1] == occurances[1][1])):\n",
    "        answer = vote(distance, k-1)\n",
    "    else:\n",
    "        answer = occurances[0]\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Data\n",
    "\n",
    "We're just going to load in the iris dataset from `sklearn.datasets` to test our code. This data set contains petal and sepal length and width data for three classes of flowers: *Setosa*, *Versicolour*, *Virginica*. Check out [this link](https://archive.ics.uci.edu/ml/datasets/iris) for more information! We will split up this data set into a test and train set and drop the labels from the test set. We will then test our data for several different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>6.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "140                6.7               3.1                5.6               2.4   \n",
       "136                6.3               3.4                5.6               2.4   \n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "75                 6.6               3.0                4.4               1.4   \n",
       "42                 4.4               3.2                1.3               0.2   \n",
       "\n",
       "     class  \n",
       "140      2  \n",
       "136      2  \n",
       "25       0  \n",
       "75       1  \n",
       "42       0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I will be using sklearn but only to grab the data and split it into a train and test set. \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = load_iris()\n",
    "Y_iris = iris_data.target\n",
    "iris_data = pd.DataFrame(iris_data.data,\n",
    "                            columns = iris_data.feature_names)\n",
    "iris_data = pd.concat([iris_data,pd.Series(Y_iris)],axis=1)\n",
    "iris_data.rename(columns = {0:'class'},inplace=True)\n",
    "train, test = train_test_split(iris_data, test_size = 0.2, shuffle = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing k = 1...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 3...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 5...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 7...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 9...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 11...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 13...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 15...\n",
      "Congrats, you have 96.67% accuracy!\n"
     ]
    }
   ],
   "source": [
    "# now let's test different values of k for some parameter tuning\n",
    "k_list = [1,3,5,7,9,11,13,15] # this way I'll get no 3-way ties\n",
    "\n",
    "for k in k_list:\n",
    "    print('Testing k = {}...'.format(k))\n",
    "    predict_set(train, test, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is pretty good. If we were being good about ML we would perform some cross validation but we are really\n",
    "# just demonstrating the method here. I would like to see how these results differ with different metrics!\n",
    "# Let's test out Cosine distance ... Note there are MANY distance measures and diving into \n",
    "# which ones perform the best could consititute an entirely separate post.\n",
    "\n",
    "cosine_distance = lambda x: (np.dot(x[0],x[1]))/((np.linalg.norm(x[0]))*(np.linalg.norm(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing k = 1...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 3...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 5...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 7...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 9...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 11...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 13...\n",
      "Congrats, you have 100.00% accuracy!\n",
      "Testing k = 15...\n",
      "Congrats, you have 100.00% accuracy!\n"
     ]
    }
   ],
   "source": [
    "for k in k_list:\n",
    "    print('Testing k = {}...'.format(k))\n",
    "    predict_set(train, test, k = k, distance_metric = cosine_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing k = 1...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 3...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 5...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 7...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 9...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 11...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 13...\n",
      "Congrats, you have 96.67% accuracy!\n",
      "Testing k = 15...\n",
      "Congrats, you have 96.67% accuracy!\n"
     ]
    }
   ],
   "source": [
    "# How does the L1-norm perform?\n",
    "L1_distance = lambda vec: np.linalg.norm(vec[1]-vec[0],ord=1)\n",
    "for k in k_list:\n",
    "    print('Testing k = {}...'.format(k))\n",
    "    predict_set(train, test, k = k, distance_metric = L1_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data and Library Implementations \n",
    "\n",
    "Let's load a new dataset to test how our algorithm compares with library implementations. We will use `scikit-learn` as the implementation library. We will also take advantage of some `magic` to see how the speed of our method compares with that of `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.444664</td>\n",
       "      <td>0.556150</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.185654</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.132492</td>\n",
       "      <td>0.351536</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.179743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.610672</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.198738</td>\n",
       "      <td>0.543515</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.113553</td>\n",
       "      <td>0.172611</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.160526</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.567010</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.334483</td>\n",
       "      <td>0.284810</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.296530</td>\n",
       "      <td>0.129693</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.542125</td>\n",
       "      <td>0.286733</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.179842</td>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.506897</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.593060</td>\n",
       "      <td>0.368601</td>\n",
       "      <td>0.617886</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.703994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.205263</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.737968</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.213793</td>\n",
       "      <td>0.137131</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.362776</td>\n",
       "      <td>0.104096</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.247504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "131  0.486842    0.444664  0.556150           0.484536   0.369565   \n",
       "160  0.350000    0.610672  0.545455           0.536082   0.195652   \n",
       "88   0.160526    0.260870  0.588235           0.567010   0.152174   \n",
       "51   0.736842    0.179842  0.663102           0.340206   0.260870   \n",
       "96   0.205263    0.272727  0.737968           0.561856   0.695652   \n",
       "\n",
       "     total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "131       0.110345    0.185654              0.207547         0.132492   \n",
       "160       0.455172    0.122363              0.698113         0.198738   \n",
       "88        0.334483    0.284810              0.660377         0.296530   \n",
       "51        0.506897    0.559072              0.169811         0.593060   \n",
       "96        0.213793    0.137131              0.018868         0.362776   \n",
       "\n",
       "     color_intensity       hue  od280/od315_of_diluted_wines   proline  class  \n",
       "131         0.351536  0.211382                      0.054945  0.179743      2  \n",
       "160         0.543515  0.065041                      0.113553  0.172611      2  \n",
       "88          0.129693  0.422764                      0.542125  0.286733      1  \n",
       "51          0.368601  0.617886                      0.769231  0.703994      0  \n",
       "96          0.104096  0.382114                      0.362637  0.247504      1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "wine_data = load_wine()\n",
    "Y_wine = wine_data.target\n",
    "\n",
    "# on this data we should be normalizing the features since the scales for different features do not match. i.e.\n",
    "# how do you compare the scale for alcohol content and color intensity?\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "wine_scaled = scaler.fit_transform(wine_data.data)\n",
    "\n",
    "# now we can combine this with the targets\n",
    "wine_data = pd.DataFrame(wine_scaled,\n",
    "                            columns = wine_data.feature_names)\n",
    "wine_data = pd.concat([wine_data,pd.Series(Y_wine)],axis=1)\n",
    "wine_data.rename(columns = {0:'class'},inplace=True)\n",
    "train, test = train_test_split(wine_data, test_size = 0.2, shuffle = True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "Congrats, you have 97.22% accuracy!\n",
      "1.31 s ± 45.8 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3\n",
    "\n",
    "# Let's check the performance of our algorithm\n",
    "k = 5\n",
    "predict_set(train, test, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check the scikit-learn implementation\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, algorithm = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "Congrats, you have 97.22% accuracy. !\n",
      "4.93 ms ± 387 µs per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3\n",
    "classifier.fit(train.drop('class',axis=1), train['class'])\n",
    "score = classifier.score(test.drop('class',axis=1), test['class'])\n",
    "print(\"Congrats, you have {:.2%} accuracy!\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holy library implementation Batman, that was fast!\n",
    "\n",
    "Well, as you can see, the `sklearn` implementation was almost a factor of $10^3$ faster than our homebrew method. The good news is our algorithm provided the same accuracy as the library implementation so... at least we have that going for us. In all reality we should have expected the library implementation to be significantly faster as it has been profesionally developed and polished over time. \n",
    "\n",
    "If we were being good data scientists we would now perform some hyperparameter tuning to get the most accuracy out of our model! But, again, since the goal of this notebook was just to get a feel for the algorithm I think we'll stop here. Not only have we gone through how the kNN classifier works step-by-step, we have also seen when it is effective! If the dataset you're working with has labeled, quantitative features in distinct groups then kNN can be a great algorithm to use to predict labels of new data!\n",
    "\n",
    "### Why *shouldn't* we use kNN?\n",
    "\n",
    "As you may have noticed. For every instance in the test set we had to compute the distance between that instance and every instance in the training set. This can be computationally intensive for larger datasets. This rubs against another problem: for large datasets we still need to store **every** point during learning. Thus, this algorithm isn't very memory efficient. It is also very sensitive to outliers as the prediction process is just basic voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
